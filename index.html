<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Kit Plummer &amp; John Scott" />
  <title>The Modern Software Engineering Enterprise</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="pandoc.css" type="text/css" />
</head>
<body>
<div id="header">
<h1 class="title">The Modern Software Engineering Enterprise</h1>
<h2 class="author">Kit Plummer &amp; John Scott</h2>
<h3 class="date">kitplummer@gmail.com &amp; jms3rd@gmail.com</h3>
</div>
<div id="TOC">
<ul>
<li><a href="#version">Version</a></li>
<li><a href="#license">License</a><ul>
<li><a href="#contributing">Contributing</a><ul>
<li><a href="#building">Building</a></li>
</ul></li>
</ul></li>
<li><a href="#introduction">Introduction</a><ul>
<li><a href="#whos-this-for">Who's This For?</a></li>
<li><a href="#introducing">Introducing</a></li>
</ul></li>
<li><a href="#tldr-nee-executive-summary">TLDR; nee Executive Summary</a></li>
<li><a href="#culture-of-change-people">Culture of Change, People!</a><ul>
<li><a href="#software-is-never-complete-sort-of">Software is Never Complete (Sort of)</a></li>
<li><a href="#open">Open</a></li>
<li><a href="#knowledge-sharing">Knowledge Sharing</a></li>
<li><a href="#polyglot-yet-mastery">Polyglot yet Mastery</a></li>
<li><a href="#devops">DevOps</a></li>
<li><a href="#organizational-positions">Organizational Positions</a><ul>
<li><a href="#the-software-architect">The Software Architect</a></li>
<li><a href="#the-chief-engineer">The Chief Engineer</a></li>
</ul></li>
</ul></li>
<li><a href="#process-and-methodologies">Process and Methodologies</a><ul>
<li><a href="#supply-chain-management">Supply Chain Management</a></li>
<li><a href="#waterfall">Waterfall</a></li>
<li><a href="#agile-processes-always-evolve">Agile (&amp; Processes Always Evolve)</a></li>
<li><a href="#technical-debt">Technical Debt</a></li>
<li><a href="#architecture">Architecture</a><ul>
<li><a href="#the-modern-software-architect">The Modern Software Architect</a></li>
<li><a href="#monolithic-versus-microservices">Monolithic versus Microservices</a></li>
<li><a href="#factors-of-modern-software">12-Factors of Modern Software</a></li>
<li><a href="#mobile">Mobile</a></li>
</ul></li>
<li><a href="#continuous-delivery">Continuous Delivery</a><ul>
<li><a href="#artifact-repositories">Artifact Repositories</a></li>
<li><a href="#release-management">Release Management</a></li>
</ul></li>
<li><a href="#development">Development</a><ul>
<li><a href="#issuetask-tracking">Issue/task Tracking</a></li>
<li><a href="#review-and-documentation">Review and Documentation</a></li>
<li><a href="#testing">Testing</a></li>
<li><a href="#style-compliance">Style Compliance</a></li>
</ul></li>
<li><a href="#automation">Automation</a><ul>
<li><a href="#test-environments-stages">Test Environments (Stages)</a></li>
</ul></li>
<li><a href="#auditing---quality-assurance">Auditing -&gt; Quality Assurance</a><ul>
<li><a href="#change-management">Change Management</a></li>
<li><a href="#separation-of-duties">Separation of Duties</a></li>
<li><a href="#separation-of-systems">Separation of Systems</a></li>
<li><a href="#password-and-access-control">Password and Access Control</a></li>
<li><a href="#cicd-automation-security">CI/CD Automation Security</a></li>
<li><a href="#process-review">Process Review</a></li>
</ul></li>
<li><a href="#logging">Logging</a></li>
<li><a href="#legacy">Legacy</a></li>
</ul></li>
<li><a href="#technology-services-and-tools">Technology: Services and Tools</a><ul>
<li><a href="#vendor-independence-tool-independence">Vendor Independence, Tool Independence</a></li>
<li><a href="#enterprise-wide">Enterprise-Wide</a><ul>
<li><a href="#ephemeralization">Ephemeralization</a></li>
<li><a href="#testing-toolkits-and-infrastructure">Testing Toolkits and Infrastructure</a></li>
<li><a href="#continuous-integration-and-the-pipeline">Continuous Integration and the Pipeline</a></li>
<li><a href="#cloud-public-and-private">Cloud (Public AND Private)</a></li>
<li><a href="#distributed-version-control-system">Distributed Version Control System</a></li>
<li><a href="#artifact-control-general-and-language-specific">Artifact Control (general and language-specific)</a></li>
<li><a href="#monitoring">Monitoring</a></li>
</ul></li>
<li><a href="#project-specific">Project-Specific</a><ul>
<li><a href="#delivery-requirements">Delivery requirements</a></li>
<li><a href="#target-matrix">Target Matrix</a></li>
<li><a href="#development-environments">Development Environments</a></li>
<li><a href="#configuration-management">Configuration Management!</a></li>
<li><a href="#integrated-development-environments-ides">Integrated Development Environments (IDEs)</a></li>
<li><a href="#the-physical-environment">The Physical Environment</a></li>
<li><a href="#the-distributed-environment">The Distributed Environment</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<p></p>
<div class="figure">
<img src="images/image_0.jpg" alt="&quot;MAM Caracas&quot;" /><p class="caption">&quot;MAM Caracas&quot;</p>
</div>
<h1 id="version">Version</h1>
<p><em>This document is built from source files, which are available here:</em></p>
<p><a href="https://github.com/kitplummer/modsofengent" class="uri">https://github.com/kitplummer/modsofengent</a></p>
<p>This particular build of the document is:</p>
<p>VERSION=0.3.0</p>
<p></p>
<h1 id="license">License</h1>
<p>Copyright (c) 2014 by Kit Plummer (kitplummer@gmail.com) Copyright (c) 2014 by AirGap.io</p>
<p>This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/ or send a letter to Creative Commons, 444 Castro Street, Suite 900, Mountain View, California, 94041, USA.</p>
<p><a href="http://creativecommons.org/licenses/by-nc-nd/4.0/" class="uri">http://creativecommons.org/licenses/by-nc-nd/4.0/</a></p>
<p>The intent in using this particular license is to allow for the continued free distribution of the works, while maintaining the content in a centralized way - providing the appropriate attribution to those who've contributed in whatever way.</p>
<h2 id="contributing">Contributing</h2>
<p>This paper started as a simple document, with feeble aspirations. It's since grown just a little bit, to encompass a much broader and slightly deeper target. And most importantly, it's been repackaged in a form that aligns with software engineering.</p>
<p>Now, it's <a href="https://github.io/kitplummer/modsoftengent">out there</a> for anyone to contribute to.</p>
<p>Please help, by commenting <a href="https://github.com/kitplummer/modsofengent/commit/6e5a4692949cf2742084aa49a7bb0688ac7528ed">in the commits</a>, by creating an <a href="https://github.com/kitplummer/modsofengent/issues">issue</a>, or forking the code base and submitting a <a href="https://github.com/kitplummer/modsofengent/pulls?q=is%3Aopen+is%3Apr">pull request</a></p>
<p>If you do contribute I'll add a <em>Contributions</em> section with a link to your contribution directly.</p>
<h3 id="building">Building</h3>
<p>I've only ever tried to build the document on a Mac and Linux.</p>
<h4 id="requires">Requires</h4>
<p>pandoc (which has some depends itself, like pdflatex)</p>
<h4 id="run">Run</h4>
<p>$ ./build.sh pdf</p>
<p>It'll spit out a new PDF file in the source directory.</p>
<p>$ ./build.sh html</p>
<p>It'll spit out a single HTML file in the source directory.</p>
<p></p>
<h1 id="introduction">Introduction</h1>
<h2 id="whos-this-for">Who's This For?</h2>
<p>This document was specifically written for technical leaders in the government space. It has evolved intentionally to be broader reaching, and to incorporate ideals and concepts that should appeal to small and medium-sized businesses as well. If you are anywhere in the continuum of software - from idea to provider - there is something to take away.</p>
<h2 id="introducing">Introducing</h2>
<p>Software IS eating the world <a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. Software is as pervasive as it gets. Companies big and small are developing software for themselves and their customers. But are they keeping up with the times - are they modern software engineering enterprises?</p>
<p>What is a modern software engineering enterprise? First, it is an organization committed to continuously delivering high quality and secure software products and services. Second, it is the collection of skills and knowledge that operates the infrastructure, designs systems, implements concepts and architectures, and monitors and maintains the whole using the optimum processes and tools at any given time. In addition, the software engineering enterprise is a measurable, repeatable, continuously improving suite of activities covering the lifecycles of all software-related objectives within and across organizations. To survive, the modern enterprise must be adaptable - flexible to the point of contortion. It must constantly learn, both from mistakes and successes not only of its own, but of the domain around them.</p>
<p>Whether your organization delivers software products, develops custom internal apps, or just operates a software infrastructure of services there is a common dependence on upstream software applications, frameworks, libraries and a plethora of formal and informal documentation. That dependence becomes a critical factor in the operation of any organization, impacting security, budget, performance stability and ability to flex and pivot where and when needed.</p>
<p>The challenge is compounded because software is constantly evolving, at all stages. Libraries depend on libraries. Frameworks depend on libraries. Applications depend on frameworks and libraries and applications. When any one of the dependencies changes it impacts all downstream software. Further, not only are vulnerabilities important, but so is generic performance. While we assume that changes upstream are making some kind of improvement, are we sure when softwares are integrated or connected?</p>
<p>The US government has tried to get its hand around this with the The National Vulnerability Database<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> (run by the National Institute of Standards and Technology and sponsored by the US Department of Homeland Security), a dynamic collection of reported software vulnerabilities, identifies the issues and relationships - to an extent. It is generally a catalog of &quot;known&quot; vulnerabilities, meaning only those that are reported to NIST and it is massive, but lacking and extremely difficult to fully utilize.</p>
<p>The speed at which software is changing also creates a massive scope challenge. The days of software being released on long-term schedules (e.g. years and quarters) has passed. Today softwares are released weekly, monthly, or at every change - in real-time. The need to release functionality to users, from a business perspective, combined with the vulnerability/security dilemma has created a storm of change, as far upstream as imaginable too. The permutations are exponential. [Sure, we are growing comfortable with offloading responsibility, opting into cloud-based/web services whenever possible. Is it possible for an organization to be completely void of consuming and operating any software? Probably not. And, the focus of this paper is the organizations who produce software - for either internal or external consumers.]</p>
<p>The modern software engineering enterprise is one that is capable of proactively managing the change inherent on software. Change management isn’t what it used to be<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>. Software engineering has come quite the ways in the past 5 years too.</p>
<p>&quot;Software engineering focuses on software development and goes beyond programming to include such things as eliciting customers’ requirements, and designing and testing software.&quot;<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> The ACM’s definition here does little to recognize the importance of the delivery process (including the management of dependencies) or operations’ requirements. So we’ll bring these aspects into the modern “enterprise”.</p>
<p></p>
<h1 id="tldr-nee-executive-summary">TLDR; nee Executive Summary</h1>
<p>The pace, scale and speed of change in the software world has created nightmare for IT shops and engineering functions within small and large organizations alike. Dependency complexity combined with the turnover of new technologies is escalating the need for greater software supply chain awareness and control. Software engineering enterprises, organizations who design and develop software solutions for internal or external consumer use, are faced with the need for modern practices and tools - before they succumb to overwhelming technical debt and security vulnerabilities.</p>
<p>It takes a combination of culture, agile and dynamic processes, and evolving organization-wide services and tools for modern enterprises to manage proactively the aforementioned challenges. The engineering culture must include analysts, developers, administrators, and operators who are continuously in control of the supply chain, performance and security requirements, and changes in available technologies. More importantly, a workforce that is happily willing to work together without barriers during the lifespan of software systems. The enterprise must engender a sense of leadership with well-defined processes that continuously evolve to enable pragmatic practices and methods - while providing audit-able control. The enterprise must adopt a &quot;right tool for the job&quot; approach, and enlist and grow the skills required to use those tools. Monitoring and measurement of all of the software must be a hard requirement, and integral in the design, implementation and maintenance of the system. Infrastructure and resources must be uniformly available for architects, developers, testers and operators to collaborate towards an efficient delivery apparatus that includes both production, development and test environments.</p>
<p>The focus of this paper is to give a brief overview of what it means to be and prepare for the modern software enterprise. Here's the short list, unordered:</p>
<ol style="list-style-type: decimal">
<li><p>Development environments that replicate production (based on captured requirements that drive or derive from production systems), allowing for continuous delivery (possibly deployment)</p></li>
<li><p>Automate the functional (performance, security, etc.) testing environment, based on dynamically provisioned resources</p></li>
<li><p>Centrally manage ingress and produced artifacts in a controlled repository for all development languages and frameworks (Java, Python, Ruby, etc.)</p></li>
<li><p>Establish an open and sharing culture for developers with a social-coding platform (internal or externally managed) like Github</p></li>
<li><p>Adopt a &quot;right tool for the job&quot; mentality - allowing projects the flexibility to go through the decision-making process for down-selection of dependencies and tools.</p></li>
<li><p>Investigate deeply, the architectural infrastructure that best suits the long-term development and maintenance of all software projects (e.g. microservices, versus monolithic applications)</p></li>
<li><p>Provide suites of tools consistently to all projects (enterprise-managed issue/task tracking, source code management, testing infrastructure, self-service virtual machines, etc.) to allow for continuity and developer confidence</p></li>
<li><p>Remove walls between functional/operational staff, bringing the right skills to the projects early - specifically developers, operators, security and qa, and business analyst early and continuously through the lifecycle</p></li>
<li><p>Design infrastructure and software for measurement and metrics-based management up-front - providing operators with the tools required to automate the feedback loop</p></li>
<li><p>Support rewrites or termination of legacy products and services when the technical debt associated with them weighs more than the replacement effort.</p></li>
</ol>
<p></p>
<h1 id="culture-of-change-people">Culture of Change, People!</h1>
<p>Change is hard. But, it must be controlled, proactively, in order for any enterprise to be able to manage against it. Deep dependencies and concrete processes can cripple an organization as requirements and inputs change upstream.</p>
<p>Diversity is complex. However diversity is the attribute that can help an organization’s culture towards not only being change tolerant, but change-driven. It is extremely critical that the culture stay diverse, open and willing to share knowledge and experience, and not inflexible to operating boundaries. While diversity is good, this does not imply that functions within the organization can avoid specialization and a learning path required to master any given domain. Also while diversity does imply breadth the distribution of an organizations skills can not be applied serially or in sequence - which create intermediate walls and barriers for progress. While a matrix organizational<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> design can help, it isn’t necessarily the only answer. The most important ideal is that the right collection of skills are applied at the appropriate times - and not just for the sake of diversity.</p>
<h2 id="software-is-never-complete-sort-of">Software is Never Complete (Sort of)</h2>
<p>While Fred Brooks, of The Mythical Man-Month fame, opined that software should only change up to a point otherwise it will never be finished<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> and that code freezes are a must, it is difficult to apply this thought across the board to modern software. This is even more true given the shift to components and frameworks where dependencies are more externalized - and constantly changing as well. That said, it isn’t difficult to imagine libraries whose functionality is solidified to the point where there is no more change possible. Also, it is possible for applications to be retired, or intentionally have a short/specified lifetime. In fact, the notion that applications should be shelved when their cost to operate exceeds their usefulness, or cost to re-implement with modern tools and methods should be acceptable. The ability to manage against these types of change requires an inherent acceptance of change, from the top leadership layers to development and operations teams.</p>
<p>The 'sort of' in the header is really a key aspect to continuous delivery. While software and its dependencies are always susceptible to change, this doesn't mean that it can't be stable. The rate of change can also be managed; it is always necessary to continuously deliver against any change. In addition, it is quite possible that code may be frozen, or even put into a legacy/archived state where no more maintenance or feature development is being done.</p>
<h2 id="open">Open</h2>
<p>In order for quality to run through as a core value of all software and engineering projects in an enterprise they should be transparent. Having projects in a &quot;shared&quot; version control system allows for “social coding” where security requirements allow, it is best - also for knowledge-sharing - that project’s source code be discoverable, and reviewable by all in the enterprise. In cases where it isn’t permissible for security reasons - as much of the code base should be managed in an up-stream dependency pattern (just like external dependencies) in order to optimize the potential for collaboration across projects.</p>
<p>It is a growing trend that enterprises also contribute open source pieces of their projects, in order to garner a greater community effort into the development and save development times and funds. There are overhead costs to this, so the impact on the enterprise and projects should be reviewed - beyond just intellectual property concerns. This is a topic that goes beyond the scope of this paper, but it is relevant to both the culture and processes of a modern environment.</p>
<p>Adding the social aspect to the life of code is also useful in an enterprise. Aside from the obvious reuse and collaboration potential, the projects could be used for HR and personnel reviews, and training and knowledge sharing purposes. Skill sets can be gleaned from contributions, identifying opportunities for future projects or even mentorship. Code as reputation<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a>, or the notion that an open environment will motivate, and reward individuals for their contributions makes a lot of sense - proven by the power of Github.com a central repository for open sources.</p>
<p>Open communications seems like an obvious attribute within any enterprise. But, it isn’t just about open doors, and open spaces. While open communication is extremely important for any environment where change is prevalent, software teams need a channel for communication that persists the dialog, and makes sharing ideas, and asking questions barrier-free. The benefit is that is both passive and active, especially if team members are geographically separated. A dialog can take place in real-time, and then other team members can contribute when they are available. In order to avoid too much noise, different channels can be used for different topics/subjects (ala old-fashioned IRC or chat).</p>
<p>Another benefit is that IT systems can now self-report into these chat channels providing data about the automation infrastructure (e.g. the state of the last build on a commit/change) which then can be discussed, or managed appropriately. It is also possible to integrate project management tools, enabling the teams to get notifications of workflow state changes, or task assignments. The transparent nature of this model is an improvement over email in that the team(s) can have a easy awareness about the overall operation of things within their purview. In short, open communications is a key part of developing and maintaining modern software: in fact it might be the first Axiom.</p>
<p>In addition to group communication, and somewhat dependent on the size of the enterprise a social feed is extremely useful in creating relationships, based on a context of interest, across teams and possibly across organizations. This is especially useful in enterprises that bridge many geographic areas and cultures. Having a &quot;Twitter&quot;-like feed can help openly permeate ideas, new technologies and provide a platform for emergent innovation<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a>.</p>
<p>Once teams and individuals operate in a more open environment, based on transparency of the project’s code base and persisted communications channels - the level of knowledge sharing grows continuously.</p>
<h2 id="knowledge-sharing">Knowledge Sharing</h2>
<p>It is much easier to talk about knowledge sharing than it is to have a culture that subscribes to it. This is caused by everything from personalities to job security. However, an enterprise that evolves process and tools, will grow the culture in the direction that it requires. It really starts with a social-centric source code repository tool set - all Github.com. While Github is one example, there a handful of options available for intra-enterprise use. By removing the barrier to entry for contributions - even if outside the project’s own development team - the enterprise establishes a &quot;search-first&quot; environment. By opening this door the enterprise can also prevent tribal knowledge where certain projects hoard - intentionally or otherwise.</p>
<p>Brown-bags, book studies, hosting meet-ups, and tech-sharing conferences all can serve to help distribute knowledge through the organization. Getting junior staff involved in the general knowledge sharing culture, by presenting on their learning process removes inhibition and helps create an accepting environment.</p>
<p>The idea of knowledge sharing isn’t without issue. In most cases it is the tools at the root of these issues<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a> rather than the concept itself. The challenge becomes one where pockets of knowledge can be created, and technologies and practices are complex enough where it doesn’t make sense to share. Enterprises have a tendency to want to create &quot;areas of expertise&quot; or “communities of practice” which attempt to create opportunities for knowledge sharing, but end up just constraining and walling off (e.g., cylinders of excellence and ignorance) the reach of the knowledge.</p>
<p>A practical method for sharing knowledge has been to fund community managers and mavens to help govern, curate and move software conversations along. These people tend to be the bridge from the developer level to the larger IT enterprise vision. But to be clear these people are not in charge of a given community but are facilitators (and coders in their own right), usually nominated by the communities themselves.</p>
<p>One challenge for enterprises is creating an environment that isn’t about squeezing out individual knowledge, or appearing to be an intellectual property blackhole. One way to manage this is to enable engineers to be more open, sharing publicly via blog posts and big-community contributions. Therefore the individual’s effort is as much about self-marketing (internally and externally) as it is about sharing.</p>
<p>It also should go without saying, but needs to be drilled home: modern software is evolving the enterprise into a continually learning machine. All resources should be tuned to deliver this maximum effect.</p>
<h2 id="polyglot-yet-mastery">Polyglot yet Mastery</h2>
<p>One of the big cultural challenges, with regard to making software inside an enterprise, is the continuous evolution of languages and frameworks/domain-specific languages. While Java, at the least the Java Virtual Machine (JVM) still has a stranglehold on enterprise development, there are many new options, and new directions for developing and operating around the JVM. The rise of virtual machines and cloud resources has also brought to the table a desire for many developers have access to modern languages. Falling out of the startup space, there is also a shift for developers, architects and operators to choose the right tool for the job - moving away from the traditional square-peg round-hole problem. Developers are quickly faced with polyglot<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a> environments. And the &quot;right tool&quot;<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a> becomes an unwinnable chase. The most important aspect is that the organization have the ability to shift, when it needs too, and via solid decision making. While we can't avoid polyglot enterprises, we can still provide the opportunity to master - more like optimize.</p>
<p>It is probably impossible to decouple organizations from the &quot;shop&quot; mentality - as they are unable to throw away investment with ease. But, as with all culture, diversity is powerful and tends to breed creativity. It is important that as projects rotate, or when new project go through the architecture and design reviews - that leadership take opportunities to look at new technologies, or options available in languages outside of the internal skillset.</p>
<p>Specialists are still required. Each layer is too complex and too dynamic for individuals or organizations to distribute across functional layers. Expertise is still required to optimize any layer. Full-stack developers, it’s a really small pool, and somewhat of a farce. As mentioned above, the complexity of each layer in the stack - at some point, and most likely towards production - will extend beyond the expertise a full-stacker would have in it.</p>
<p>Example: Consider the layers in a stack of anything beyond a super simple web application. Starting at the bottom - you have at least one data store, and likely more than one type (e.g. relational for records, document-oriented for metrics, and possible key-value for configuration). Next is probably some kind of messaging/asynchronous processor, followed by the generic application layer. The top, or front-end is another, where there may multiple types - each requiring it's own language (e.g. web, mobile, etc.). There are so many disparate technologies there, that would prevent anyone from being an expert in all of them.</p>
<p>All team members must share a common understanding of operational requirements - and must be able to communicate across the matrix. So, even if not an expert some awareness of the different technologies and languages is beneficial.</p>
<h2 id="devops">DevOps</h2>
<p>All enterprises are software enterprises now. It’s the nature of the world we live in. This isn’t to be confused with traditional information technology (IT) functions either.</p>
<p>DevOps is, simply, the cross-pollination of developer and operations skills, within a central culture that removes the barriers between the traditionally separate groups. The longer winded definition<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a> is nuanced and openly debated still.</p>
<p>The key takeaway is basic: it IS really important for different roles to collaborate to arrive at a common understanding, or system architecture. Developers must understand operational requirements. Operators and system administrators must understand security and policy requirements. And that common understanding requires input from everyone in the engineering to operations continuum (and the consumer or user).</p>
<p>There are a few enterprise practices that are generally missing from the DevOps conversation, but can't be avoided: security and quality assurance. As long as these stakeholders are involved in the development processes, along with operations at an early stage, and then continuously through production then DevOps is an achievable thing.</p>
<p>DevOps usually has connotations for automation, metrics and monitoring, and the delivery practice. While they fit, they can be decoupled - an enterprise doesn't need a DevOps position to incorporate all of the espoused goodness.</p>
<p>Culture must be paired with processes and methods that enable, yet control change. Without an infrastructure of flexibility, technical debt will amass. The process for change must be understood, and not encumbered by personal or organizational intelligence.</p>
<p></p>
<h2 id="organizational-positions">Organizational Positions</h2>
<p>As with any operational environment the software engineering enterprise also requires an administrative and functional hierarchy. Aside from a reporting aparatus the administrative structure serves to provide feedback and mentorship opportunities, promoting (and rewarding) personal growth. The functional structure is required to manage, as in oversight, the organization's many projects and efforts from a technical perspective - aligning skills and business requirements. There a couple key roles in this regard worth identifying and discussing: the software architect and the chief engineer.</p>
<h3 id="the-software-architect">The Software Architect</h3>
<p>The software architect role is one centered around skills, usually in a specific area - databases, networking, metrics and monitoring, services. The architect should have a solid understanding of industry standards, and practices as well as a solid ability to relate them to business requirements. Architects are critical in young projects, projects requiring major transitions in functionality, and efforts where integration with external systems is required. In most cases architects are practitioners as well, having an ability to work within cross-functional teams to coordinate and make decisions on critical points regarding technical options.</p>
<h3 id="the-chief-engineer">The Chief Engineer</h3>
<p>The role of the chief engineer is one that has transcended very large federal engineering projects that include many sub-contractors, and many different sub-projects, all leading up to a top-level distributable delivery within a complex process methodology. The role of the chief engineer in an IT organization is not at all dissimilar. And to a degree this role is similar to what is covered by an Chief Information Officer or Chief Technical Officer. The main function of the chief engineer is oversight - of all things people, process and technology. The intent of the oversight is simple, deliver quality on/in time, in the most maintainable means possible with least amount of duplication or technical debt.</p>
<p></p>
<h1 id="process-and-methodologies">Process and Methodologies</h1>
<p>The discussion of process and methodologies is an infinite one. But there are a few irrefutable truths - all centered around organizational agility and changing markets.</p>
<p>The modern software engineering enterprise requires speed and extensibility in order to deliver on time, the right product, with the ability to assure quality. The use of complete Agile methods, or even just agile practices will help an organization support cultural requirements, and allow for the appropriate technologies to be used - nevermind an opportunity for feedback to drive the continued development of products and services.</p>
<p>Master planned, but project tuned - organizations need to ensure that each project has the flexibility to optimize the processes for their specific needs. That said, there's still a need for the organization to provide oversight and controls around the path to production to ensure all objectives are met.</p>
<p>There is a lot of discussion about &quot;lean&quot; and whether or not Agile methods can be combined with more rigid process structures (e.g. CMMI). It is probably safe to say that any organization can tailor any suite or processes or methods to meet their needs if the modern enterprise is willing to invest, and reinvest in the evolution of all efforts - especially when they work towards a culture of change management.</p>
<p>There are a few focused aspects that are discussed in this chapter: supply-chains, waterfall and agile, technical-debt, architecture, development, and audit - as they all impact the ability for an organization to accept and control change.</p>
<p></p>
<h2 id="supply-chain-management">Supply Chain Management</h2>
<p>The accelerated usage of software-based systems requires a well-defined, visible, controlled and optimized supply chain pipeline to control risk, costs and deal with the increasing size, scale and tempo of software change. Increased software usage challenges existing methods for how an organization manages source code for use and inclusion into military systems as well as its ability to adopt code changes in an expeditious manner. The software supply chain needs to being managed as an enterprise service catering to developers and software operators.</p>
<p>Current supply chain processes are significantly challenged by the speed of software change: standard practice is to let individuals or programs handle it themselves without making that supply chain available to the rest of the organization. This negatively impacts getting patches or updates to fix identified issues, decreasing operational effectiveness and increasing risk. Software systems software are evolving at accelerated speeds as well as having additional needs for vetting such as using multiple dependencies with libraries, firmware code and various modular build frameworks coupled with an increasing preference to using open source software. And since software is everywhere, not having a process and high speed governance on what an organization needs to operate should be considered negligent.</p>
<p>One evolving trend in the open source space, is the concept of dependency management - at the language/library level as well as application packages (common in Linux distributions). The idea of software supply chains isn't new, but the actual handling of external or upstream dependencies as raw resources isn't typically found outside of highly-secure, mission/life critical applications.</p>
<p>For the modern software engineering enterprise, there's a need to be able to manage the ingress of software (i.e. languages, libraries, frameworks, systems, etc.) in a way that each artifact is vetted against the source - ensuring that it is what it is supposed to be and hasn't been tampered with.</p>
<p>Software artifact provenance is extremely important, for license compliance, as well as just ensuring that an organization knows what it is putting in the software it builds. A the number or projects, or the complexity scales up for applications under development - so does the need to maintain a manifest of all software included. It is also important that downstream engineers have a channel for getting notified when updates are available internal to the organization.</p>
<p>Managed repositories are required to centrally control the access both directions. The use of central repositories is discussed in more detail below.</p>
<p></p>
<h2 id="waterfall">Waterfall</h2>
<p>For most software engineers the thought of waterfall-based projects means slow progress, driven by lengthy requirements and design steps - the world of systems engineers and big-design-up-front (BDUF). For some projects this is an inescapable methodology due to delivery, customer-driven requirements and contracting, but note: even in enterprises that are bound by waterfall-based methodologies, continuous delivery is still possible. In these cases, 'production-ready' is the objective. This means that deliverables should be ready for shipping at any specified point in time - daily or weekly. Even if the end product is only delivered to customers on a quarterly (or longer) schedule any subcomponents can be pipelined to points in each stage, making it ready for consumption or testing downstream.</p>
<p>Ultimately, within waterfall there is a greater emphasis up-front and during integration. By having components readily available (automatically built and tested) they can be continuous integrated in systems and functionally tested still. The point being that the actual methodology of design, development and delivery is only benefited by being capable of managing against change and automating as deeply as possible towards continuous delivery of artifacts.</p>
<p></p>
<h2 id="agile-processes-always-evolve">Agile (&amp; Processes Always Evolve)</h2>
<p>Agile methodologies have been a boon for software engineering and organizations who are required to be responsive to their customers (and the market in general). The ability to drive development in the direction of the highest value is incredibly important.</p>
<p>The methodology itself, isn't as important as many of the pragmatic practices<a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a>. Practices like planning and continuous integration combined with notions like developer respect, and continuous delivery help raise agile to places where it's a requisite - even when combined with other process-stringent requirements.</p>
<p>But, the most important aspect for the modern software engineering enterprise is the need for the processes to evolve, based on stakeholder feedback, just as softwares being engineered do. Testing requirements change, customers themselves come and go, and the software artifacts evolve - requiring that the processes be adaptable and extensible to continue towards optimization. Agile methods must be agile themselves.</p>
<p>One component of agile often gets shortchanged is testing - even as it is the most critical piece. Developers often see testing as a basic function, that ends with a minimal set of integrations (do their developed interfaces meet the specifications?). Because of the void that is created once software successfully completes integration testing - developers wash their hands and move on to the next task. Generally accountability for the operation of the software is released to operations staff, or an external customer. The challenge is that it is rare for software to behave the same in development and test environments as it does in production (or in the hands of the customer). Developers typically focus on 'happy' paths, based on their understanding of the target environment(s). Enter DevOps, as discussed in this paper, as a cultural movement to establish better relationships between all stakeholders (in the engineering continuum).</p>
<p></p>
<h2 id="technical-debt">Technical Debt</h2>
<p>The ideas behind the notion of technical debt<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a> aren't necessarily new, they are just now being integrated into risk management planning, and development and generally IT operations. The basic premise is that every project, through decisions on technology, feature priorities, delivery pipelines, and feedback cycles incurs debt (in the form of more work to be done usually later) over time. Of course some debts are manageable and some debts end up out of the control of project staff, and oversight efforts. Upstream changes, new technologies, new processes, new requirements and most often, changes in priorities will effort a projects level of debt. Technical debt ultimately acts like a brake on software projects, can stop or slow new feature development, limit inclusion of new ideas, or worse limit how fast a software teams can fix bugs or glitches.</p>
<p>One of the challenges for the modern software engineering enterprise is how to limit technical debt as much as possible. While likely impossible to not incur debt, the ability to identify it early, and understand the impacts will affect the overall health of the project.</p>
<p>In order to best manage technical debt, all stakeholders must understand the project's requirements and immediate priorities. With a common understanding the project team can then work to ensure the backlog of work is managed in advance of work allocation - creating tasks for investigation, identifying costs for change, costs for not changing, and rationalizing termination or complete rewrites.</p>
<p>Maintenance is probably the biggest factor associated with technical debt, that isn't fully understood as the products or services are released. In a lot of cases, what was the optimum solution for production performance isn't the easiest thing to maintain - incurring operational costs, and slowing developmental advances. Refactoring for maintenance can be really difficult, and eventually requires higher-level staff (and thus more debt).</p>
<p>Other major debt concerns are upstream dependency health, technology evolution, platform and operating system requirements, and lack of extensibility.</p>
<p></p>
<h2 id="architecture">Architecture</h2>
<p>Underlying any modern software system is a collection of efforts towards defining that system to a set of requirements. Modern systems must be architected in the same way as traditional systems, from a process perspective. However, the landscape had changed - and we are wiser. Complexity<a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a> is biggest risk to a system. The greatest aspect of Agile is the desire to deliver only what is of value, constantly re-evaluating what is the greatest priority. From an architectural perspective the system under development must allow for that to happen. This isn't without challenge. Technology decisions must be made early, sending the project in a direction. By managing from a complexity perspective teams can often avoid bad directions early.</p>
<p>Support for usability, extensibility, maintainability, deliverability, testability, and the list goes on and on. But, there a few that stand out in the modern environment - most are obvious<a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a>. Scalability is probably the least understood, even in the modern software engineering world. But, scalability is the one that translates to quality customer services at the most efficient/economic effort.</p>
<p>The pinnacle of the modern engineering enterprise is delivery - deliverability is a key parameter and must be applicable to any software pipelined through the organization. But one new key feature that has become critical to modern software is monitorability: software systems must be designed and developed with the appropriate instrumentation hooks, that allow for monitoring.</p>
<p>The biggest enabler for the modern enterprise is also the shift towards modern architecture and tools that abstract complexity such as 'the cloud'. While the cloud has created various opportunities for deferring data center costs, it really hasn’t affected software architecture as much as one would think. While new technologies like map-reduce (Hadoop) and NoSQL databases (Mongo, Cassandra, Couch, etc.) are designed for distribution across an enterprise (public, private or hybrid) they don’t necessarily apply to the standard of enterprise applications and services. In addition, the evolution of Platform as a Service (PaaS) has yet to really be scaled to a point where it is readily available for on-premise operations. There is definitely effort to make this happen, and to support the continuous delivery methods espoused in this paper.</p>
<h3 id="the-modern-software-architect">The Modern Software Architect</h3>
<p>There used to exist a standard path of career progression in the software and IT industries. One started as a developer or an administrator, and progressed into an analyst role. From there, one would progress into either a management role, or an architect. Architects are created by progression, rather than trained academically or otherwise. It used to be that an architect was a specialist in some layer of the technology stack, or a generalist aware of the enterprise’s infrastructure and business requirements. Today, thanks in large part of the adoption of Agile methods, architects have become torchbearers for business objectives and entrepreneurial spirit.</p>
<p>Not only are architects responsible for decision processes, system requirements, technology awareness, and coordinating effort, but also market opportunities, business constraints, customer service and satisfaction, security and quality, and delivery pipelines. The modern software architect is integral to enabling the modern enterprise with the skills and abilities required to operate continuously.</p>
<p>Monitoring of trends is probably one of the most important tasks of any modern architect. This applies to both operational and industry/upstream trends. Architects need to be very attuned to the production environment, ensuring that representative tests and requirements are driven down to development teams. Architects need to also be aware of the complexities associated with the deployment and release processes, to ensure that services and artifacts can be continuously made available.</p>
<p>Having a sense of where industry is going isn’t the easiest thing to do - technologies are coming and going at a remarkably fast rate. However, it isn’t hard to get a feel for the general direction of things, specifically high-level trends (e.g. NoSQL, containerization, web frameworks). Architects must constantly apply both of these kinds of trends to the products and services which they are associated with - with high regard for technical debt opportunities.</p>
<h3 id="monolithic-versus-microservices">Monolithic versus Microservices</h3>
<p>One of the most active discussions in the architectural space is that of microservices versus monolithic applications. While services are not anything new - with Service-Oriented Architecture (SOA) filling the enterprise scene in the mid-to-late 2000’s - the notion has gained great momentum, rolling with the rising tide of cloud computing. Where SOA focused on heterogeneous and enterprise-wide services orchestrated together to complete business transactions, microservices are smaller software components that are derived from a single application and run as discrete processes. Microservices aren’t to be confused with modular applications (which is a similar but different discussion). Monolithic applications on the other hand are somewhat a recourse from the fallout of the SOA storm, where applications are encapsulated in a single, transportable unit and deployed to an application server (environment). Within the encapsulation that applications includes the multiple user interfaces, business logic and the data structures/models. Microservices are interesting as they help break down the complexity of applications, naturally towards atomic units with concrete interfaces. This allows for greater reusability and scalability - as well as much simpler for development, testing and delivery. From an architectural perspective more work is required early in the topology for microservices, but then more freedom is available for developers to optimize on tool/language, library and framework decisions. This freedom is the key attribute with regard to continuous delivery moving forward.</p>
<p>One of the most important aspects in an architecture is the role and requirement for network interconnectivity - something that is very difficult to control, though easy to monitor. Regardless of the application architecture, the connectivity requirements must be well understood and channeled as requirements so that all stakeholders can understand the operation impact of early design decisions.</p>
<p>Here’s a few links on the discussion:</p>
<p><a href="http://martinfowler.com/articles/microservices.html#AreMicroservicesTheFuture" class="uri">http://martinfowler.com/articles/microservices.html#AreMicroservicesTheFuture</a></p>
<p><a href="http://microservices.io/patterns/microservices.html" class="uri">http://microservices.io/patterns/microservices.html</a></p>
<p><a href="http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html" class="uri">http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html</a></p>
<p><a href="http://highscalability.com/blog/2014/7/28/the-great-microservices-vs-monolithic-apps-twitter-melee.html" class="uri">http://highscalability.com/blog/2014/7/28/the-great-microservices-vs-monolithic-apps-twitter-melee.html</a></p>
<p><a href="http://www.chrisstucchio.com/blog/2014/microservices_for_the_grumpy_neckbeard.html" class="uri">http://www.chrisstucchio.com/blog/2014/microservices_for_the_grumpy_neckbeard.html</a></p>
<h3 id="factors-of-modern-software">12-Factors of Modern Software</h3>
<p>One of the challenges with a shift in architectures, or even simple proofs of concepts, is the need to understand why the patterns are patterns, and how conventions are installed. With a return to a services-centric mindset many architects and developers are looking for ways to 'checklist' their applications against the modern standards. Operators are wondering if the evolution of the data center and cloud services is being recognized as requirements in new developments.</p>
<p>A group of developers and operators combined their wisdom in a single document as a collection of 12 aspects a modern software applications should cover<a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a>.</p>
<p>The 12 factors are:</p>
<ol style="list-style-type: decimal">
<li><p>Codebase - one codebase tracked in revision control, many deploys</p></li>
<li><p>Dependencies - explicitly declare and isolate dependencies</p></li>
<li><p>Config - store config in the environment</p></li>
<li><p>Backing Services - treat backing services as attached resources</p></li>
<li><p>Build, release, run - strictly separate build and run stages</p></li>
<li><p>Processes - execute the app as one or more stateless processes</p></li>
<li><p>Port binding - export services via port binding</p></li>
<li><p>Concurrency - scale out via the process model</p></li>
<li><p>Disposability - maximize robustness with fast startup and graceful shutdown</p></li>
<li><p>Dev/prod parity - keep development, staging, and production as similar as possible</p></li>
<li><p>Logs - treat logs as event streams</p></li>
<li><p>Admin processes - run admin/management tasks as one-off processes</p></li>
</ol>
<p>It is definitely worth going through the deep dive for each section on the 12-Factors site as the overarching focus covers deliverability, scalability and monitorability.</p>
<h3 id="mobile">Mobile</h3>
<p>Mobile apps still require infrastructure and a common development environment, and should be treated in the same way as any other architecture - from a process and delivery perspective. In addition, aside from the mobile platform’s SDKs which are regularly released, there are still dependencies to manage, and a somewhat more constrained development and test environment.</p>
<p>Each of the app stores has a different suite of processes...automate the delivery to their endpoint as much as possible. Identify technical debts as quickly as possible, and ensure that as much of the mobile-specific development is documented and understood across the team(s). While there are specific skillsets required for mobile development, it is critical that the modern enterprise attempt to limit the segregation of the development efforts.</p>
<p>Mobile requirements for applications have roared onto the scene and are a great example of how technical debt has built up in some legacy systems since these systems were never built with an eye to be slimmed down for wireless delivery.</p>
<p></p>
<h2 id="continuous-delivery">Continuous Delivery</h2>
<p>The concept of continuously delivering a product isn’t anything new in the hardware world. Demand drives supply, and manufacturing supports. Continuous Delivery<a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a> of software in practice isn’t really new, but the formal description of the processes is. Where continuous integration encapsulates a realtime testing function during the development process, continuous delivery practices pick up and focus on the steps required to move the software under development to a deployable state. There is also the concept of continuous deployment which causes discourse<a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a> within the community that cares.</p>
<div class="figure">
<img src="images/image_1.jpg" alt="Continuous Delivery..." /><p class="caption">Continuous Delivery...</p>
</div>
<p>Simply put, continuous deployment is the next step - automating the process of moving every new deployable artifact to the &quot;production&quot; environment. In many cases this could mean a “dark” side, that isn’t available directly to consumers yet. The deployments can accumulate, and then have a manual operations step that switches the production environment from “dark” to “light”, exposing all of the changes to the consumer. As the illustration highlights, it is general a business decision - removed from the developers or even operations to make the changes available. The notion of continuous deployment is better applied to web-based or even mobile applications than some other kinds of software (e.g. embedded, libraries, and components that conform to a specific release cycle) due to the nature of small changes and live updates on servers.</p>
<p>Focusing on continuous delivery for a moment, and accepting that the end state is an artifact that is acceptable for deployment. For most, this will mean that levels of testing beyond unit and integration are required. Acceptance Testing is the normal process that verifies (are we building the thing <em>right?</em>) and validates (are we building the <em>right</em> product?)<a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a> during the final tasks of a development phase. In order to make this stream continuous we need to minimize the amount of human effort injected into these tasks. It might not be easy to completely automate these tasks, but it also might be hard with the introduction of the appropriate tools. Validation is the tougher of the two - but in actuality it might not be something that needs to be executed with each change, only to do a final release in terms of deployment. Verification, while simpler from an automation perspective, essentially is dependent on the test harness to do the verification.</p>
<p>[Must read: &quot;IT Managers Guide to Continuous Delivery&quot; - <a href="http://go.xebialabs.com/IT-Managers-Guide-to-CD.html" class="uri">http://go.xebialabs.com/IT-Managers-Guide-to-CD.html</a>]</p>
<p>A couple of points on acceptance testing as an automation step. Yes, automated testing does require more development. But, more importantly, once the tests have been written (and proven) they remove the opportunity for human error - specifically from a regression perspective. If the verification CAN happen during an every-change process, then it is more likely that the quality of the software under development will stay true - or improve, if the test harness is continuously refined as well. Documentation, in terms of accurately writing the &quot;definition of done&quot;<a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a>, is extremely important - especially if it is something that can be used to verify the software functionality in an automated way. This is a great highlight to the importance of the relationship between tools, processes, and the people that use them.</p>
<p>While outside of the scope of this paper, there are various processes and tools worth mentioning that can be defined for enterprise-wide use that help bridge continuous integration with continuous delivery. Test-Driven Development (TDD)<a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a> and Behavior-Driven Development (BDD)<a href="#fn23" class="footnoteRef" id="fnref23"><sup>23</sup></a> have gone a long way to improving the quality of both the software under development and the development processes themselves. The processes and tools help distribute the ownership for quality across all of the functional players.</p>
<p>While silly this BDD example shows how to write executable/testable functional requirements. It is language independent so the test can be applied to applications of all different runtimes.</p>
<p><code>Feature: Belly</code></p>
<p><code>Scenario: a few cukes</code></p>
<p><code>Given I have 42 cukes in my belly</code></p>
<p><code>When I wait 1 hour</code></p>
<p><code>Then my belly should growl</code></p>
<p>One potential issue that I want to diffuse quickly - is that achieving continuous delivery isn’t easy, and improving the quality of both the software and the pipeline requires higher organizational IQ. Yes, there is an investment required to automate functional tests. Not investing will just create technical and human debt, and without the automation in hand scaling will be painful and costly. By pushing the testing processes to the front of the lifecycle, quality is embedded into the whole lifecycle, not just a step late in the game. In addition distributing the responsibility for quality to all of those that touch the solution is the right idea.</p>
<p>One of the paramount aspects to the modern software engineering enterprise is the need to remove the barrier between life-cycle specialists. Security personnel must be integral to the software design steps - throughout the life of a product or service. Operations personnel must be involved, early and continuously, working with developers, security, quality assurance to define the requirements from a performance, monitoring and measurement perspective. Tests and test harnesses require the involvement of all stakeholders, otherwise functional requirements become afterthoughts and thus unmaintainable in a continuous process.</p>
<p><em>[**Key point: production/operational requirements, security, delivery, testing, etc. aren’t afterthoughts. Engineering firms get it. Enterprise IT project managers don’t. Need those people involved throughout the development lifecycle.]</em></p>
<p>The stream of testing possibilities is fairly limitless. In many cases lots of different schemes can be run in parallel workflows, reducing the time to final artifact publication. Performance, penetration, license compliance, target-specific environment, and extended functionality testing can be distributed and managed asynchronously. It is also possible to external testing - to remote sources (using their APIs) which can help ramp up testing during peak release periods or for new targets.</p>
<h3 id="artifact-repositories">Artifact Repositories</h3>
<p>Delivery implies that someone transfers something, generally ownership of that something. Let’s talk through a simple use case, where something is developed in-house and used by multiple development teams downstream. Independent of the language being used, the development teams need to control their dependencies to prevent transient changes in the composition of their deliverable. How do they ensure that all developers are working with the same exact configuration? Most languages have a built-in dependency management tool that is integrated into a build suite. So they just define the version of the dependency. But, the build tool needs a centralized location to get the dependency. Or if the language doesn’t have that capability the developer needs to be able to download it, possible for each target environment where the build happens. This is also doubly important to ensure that the artifact is indeed the one provided by the true source. By using a dedicated, and centralized artifact repository each artifact can be signed (rubber stamped as OK) and then consumers can manually or automatically verify the certifier and artifact.</p>
<p>In the landscape of ITIL<a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a> the idea of an artifact repository is defined as the Definitive Software Library<a href="#fn25" class="footnoteRef" id="fnref25"><sup>25</sup></a>. The repository &quot;ensures that only correctly released and authorized versions are in use.&quot; Seems pretty simple. But, again it is the combination of tools, process and people that make it reality.</p>
<p>The most common example in the developer area is Maven which is both a build tool and the centralized repository for Java. The central repository is organized by a commercial entity (Sonatype) which controls the input into the repository - with a basic adjudication of who is allowed to submit artifacts. This bellybutton-to-push scheme has seemed to work without issue since its inception, and when the community pushes (e.g. for HTTPS support) the response has been quick. Maven Central’s network of mirrors has done really well to scale and distribute the dependency fetching process for developers all over the world. There are a handful of open source and commercial repositories for use on-premise, allowing enterprises to proxy Maven Central and store their own artifacts (including support for many languages beyond just Java). Most languages have a similar setup, but not necessarily with the same levels of security, or any adjudication at all - opting for the community to self-regulate the artifacts. While, in theory this is good as it allows for all options to be available and engineers to be selective. The downside is massive, as anyone can upload malicious artifacts that can become dependencies.</p>
<h3 id="release-management">Release Management</h3>
<p>While ITIL describes a need to have a dedicated release manager, I’d have to disagree - as this goes directly against the optimal flow of continuous delivery and realistically isn’t scalable. This isn’t to say that the practice of release management gets eschewed. Quite the contrary, release management is an integral piece, however in a modern enterprise we need to be able handle this in an automatable, repeatable, and trackable/auditable way.</p>
<p></p>
<h2 id="development">Development</h2>
<p>The processes associated with the actual development of software should be transparent across the enterprise. Enterprises should endeavor to cover everything from requirements management task tracking to the required testing and delivery automation required to move software through repeatable process steps.</p>
<h3 id="issuetask-tracking">Issue/task Tracking</h3>
<p>The art (read engineering discipline) of managing requirements, issues and tasks has not been lost completely. Even as Agile proponents harp against BDUF (&quot;Big Design Up Front&quot;), there has to be some thought, by somebody, into complexity, priority, relationships and architecture. First, there needs to be a standard, universal location and tool suite for the management of all that. It is important that as engineers and operators move from project to project, that there is familiarity in the methodology and workflows.</p>
<p>With an issue/task tracking tool in place (JIRA has become the most widely used) teams can collaborate in realtime against requirements that cover the spectrum of ‘ilities’ - security, maintainability, measurability, extensibility, portability, deliverability, etc.</p>
<p>The collection of issues and tasks serve as a documentation base, one that can be prioritized, rewritten or redirected as necessary over the life of the software system. It provides the history, present and future for the system, in a single location.</p>
<h3 id="review-and-documentation">Review and Documentation</h3>
<p>Many open source projects use a method of gates for managing contributions to the source code base. Modern source code management (SCM) systems, especially distributed ones (e.g. git) have an inherent staging apparatus where and when code can be reviewed before being applied. There are also many tools that can be used in conjunction with the SCM system to provide an annotation function to the changes, allowing developers to communicate about them in a self-documenting sort of way.</p>
<h4 id="code-reviews...">Code Reviews...</h4>
<p>There are many reason why code reviews should be required. Most of which are listed in a great CIO article<a href="#fn26" class="footnoteRef" id="fnref26"><sup>26</sup></a>. But, the biggest problem, related to the code itself (not the personal or social kind) is that reviews require context and expertise. It is generally very difficult for those not in the know to not just review for &quot;smells&quot; (NOTE: http://blog.codinghorror.com/code-smells/ ). Coordinating reviews against the small changes associated with each code commit (pull requests in the world of Git). With a distributed version control system, and a contribution path that includes a central repository it is possible to integrate a simple “review” step as part of the code submission activity. Breaking the reviews into easily reviewable chunks can help associate the context. Note, a code review is not a substitute for a design review - where the implementation plan is to be reviewed.</p>
<p>The code review should be supplemental to any static or style testing during the build. In addition, each code review should review the associated tests that verify the change is being testing appropriately. There are lots of really good ways to ensure time isn’t wasted during reviews (NOTE: http://java.dzone.com/articles/dont-waste-time-code-reviews ), but the most important thing to remember is that they should be planned, and guided by a team leader.</p>
<h3 id="testing">Testing</h3>
<p>Testing is an infinite subject with regard to software. Given the dynamic nature of software, any single change could cause a cascade of changes. Ensuring quality in this environment requires rigorous, and continuous testing - with as little impact of developer efficiency as possible.</p>
<p>It starts with testing as part of the development process, early when requirements are being analyzed, stories written, and with all stakeholders involved in defining the appropriate strategy.</p>
<p>At the root of the testing strategy is a continuous integration (CI) process, which should be used to automate low-level unit and integration tests against the codebase. Developers should be required to ensure that the CI process is uninterrupted. Most CI tools have the ability to connect directly to the SCM tool, and kick off the test suite for any change in the code repository. Developers should have access to this environment to be able to create new build and test &quot;jobs&quot; for anything work in process (against an in-development branch in SCM).</p>
<p>As mentioned above developers should be writing tests as part of their implementation process (ala TDD/BDD). It is definitely easier for developers to write the unit and integration tests during the process of development. It is easier for testers and quality assurance folks to write the specifications as part of the requirements process (during incremental cycles) rather than post development.</p>
<p>It is worth noting that increasing the rate of testing per changes does have a few challenges. Running a continuous integration process for each individual change could be a burden on system resources, or just not realistic from a time-to-test the software perspective. Because of this it is important for all stakeholders to understand the impact on a serial process, and the benefits of parallelizing the process. In many cases it is easy to streamline the testing process, splitting up different tests to be run in parallel. But, this has to be designed into the system and generally requires a automation/workflow engine to handle the tasks appropriately.</p>
<h3 id="style-compliance">Style Compliance</h3>
<p>Just as it is important for the enterprise to have a common suite of management tools, ensuring that code and documentation conform to a standard/accepted style and format will create consistency across projects. While it might seem inconsequential to a given project, it is the overarching level of organizational quality that is truly impacted. Style guides and documentation requirements can help to ensure the maintainability of software overtime, and provide a &quot;smell&quot; opportunity during reviews and testing - ‘if that piece of code requires that much commenting, it probably can be simplified’.</p>
<p>Style guides can also help enforce the ability to auto-generate documentation (for APIs and SDKs).</p>
<p>&quot;It’s important that everybody on the team participates in creating the style guide so there are no misunderstandings. Everyone has to buy in for it to be effective, and that starts by letting everyone contribute to its creation.&quot; (NOTE: http://www.smashingmagazine.com/2012/10/25/why-coding-style-matters/)</p>
<p>It is possible to test for style during both the SCM process (pushing up changes) and CI (during a build, or packaging process). There are style tools for just about every language, and it is important to note that each will have its own idioms - but, as a general rule the basic styles should be applicable across each.</p>
<h2 id="automation">Automation</h2>
<p>(scale, speed and human error)</p>
<p>This is probably the single most important concept of this paper. As the concept gets into the &quot;how to&quot; later in the paper, the ideas will become concrete.</p>
<p>The greatest risk to any software project is all that is unknown. Developers can only control the decisions they make, and the code and tests they write. The decisions are made on a state, at a given point in time. But like everything in the software world - the dependencies, test harness, networks, and operating systems (to include configurations) are all changing to keep up with security and performance requirements.</p>
<p>The only way to ensure that decisions on incorporating external changes are good ones are to continuously test the effect of the changes. Obviously this can be an ominous effort - given the permutations possible. By combining Configuration Management (CM) tools and techniques with the typical CI process we can continuously make small modifications to the system loadout, changing an upstream version dependency, and operating system configuration, or changes to the code, run the tests and verify that no regressions are introduced. But, what if the dependencies are managed centrally, then how do we get the changing artifact into play? In most cases this a simple request made by a developer to get the artifact into the enterprise’s repository.</p>
<p>In this case, the request should trigger an automated workflow to fetch the remote dependency, validate it’s source and functionality - and possibly run some basic component analysis (does this version have any known vulnerabilities, etc.) and then deploy it to the enterprise repository. Once the artifact is available the developer can make the build, code, or configuration modifications necessary and run them within an isolated sandbox (detonation chamber) and verify the results.</p>
<h3 id="test-environments-stages">Test Environments (Stages)</h3>
<p>As software grows in complexity so do the test environments. In some cases the time to run automated tests makes them somewhat inconvenient for developers to use during their development tasks. While it is generally possible to run a specific test manually, it is still important for developers to get feedback on whether or not their changes created regressions anywhere in the system. Also, many software systems include UX components, which also take a long time run tests against. By automating the UX tests, and potentially being able to parallelize the execution of them it is possible to integrate them into a typical CI process - creating better, instant feedback for developers. The same can be done for security reviews, performance analysis, and any other functional (interfaces to systems) testing.</p>
<p>The modern software engineering enterprise is one that is equipped to scale out the testing process - to include dynamically creating target environments for testing. For example, imagine a basic Java application that must be able to run against a specific version of Oracle’s and the Open JDK on MacOSX, Linux and Windows. It is one thing to be able to handle those environments, but what about all the different versions of the OSes, and different library configurations on each. By using solid CM practices, development and operations staff can provide configuration templates to development teams for testing these different environments - in parallel, and without human interaction.</p>
<p>The key to all testing is trust and repeatability. It is extremely important that both the manifest that defines the system-under-test and the results are maintained so that traceability is provided against any change. It is also important that it is understood that the information isn’t used for blame, but rather for providing both developers and operators with the information needed to continuously improve the software and the delivery of the software over time. Virtual machine-based environments make this even easier as systems can be snapshotted and archived (in the event they are ever needed in the future). And if the system configuration is captured appropriately it can be reproduced in local development environments as well.</p>
<p></p>
<h2 id="auditing---quality-assurance">Auditing -&gt; Quality Assurance</h2>
<p>When we started writing this paper we had ‘Auditing’ as a sub-topic to ‘Development’. In doing some research on where organizations were moving with the auditing and internal control tasks it became clear that this wasn’t a function that can just be related to the engineering process - either dynamically (DevOps) or statically (waterfall). As mentioned above, oversight is a critical function within an enterprise to ensure the culture, processes and technologies are all aligned. The oversight task alone creates a special purpose auditing function - where there is a greater requirement for staff to ensure business objectives are met. The audits are as much about ‘compliance’ as they are about creating an environment for the culture and teams to grow in the right directions. Evangelism is required, and auditors and QA personnel need to be injected into the development processes continuously to evolve in the same ways.</p>
<p>There is also a need for development and operations staff to assume some of the qualities of the auditing positions. Monitoring both the delivery systems and processes for opportunities to improve are essential to sharing the culture of change with quality assurance personnel. From an auditing for quality and process management perspective there are a few key areas that the modern software engineering enterprise must address continuously, and engage all stakeholders with: managing change, control of duties, access control, process (automation) security and general process documentations.</p>
<p>Continuous improvement doesn’t require CMMI<a href="#fn27" class="footnoteRef" id="fnref27"><sup>27</sup></a> for enterprises to have a plan. There are certain enterprises that will have to comply with various industry standards (SOX, etc.) in order to operate. However, the underlying culture of development and operations should be based on continuous improvement via continuous delivery and direct feedback. Stringent auditing and exam requirements should preclude continuous delivery cycles either<a href="#fn28" class="footnoteRef" id="fnref28"><sup>28</sup></a>. This is even more true if the enterprise’s time-to-market windows are short. Business requirements should drive processes that make sense, and auditing should be a tool to validate processes not impede delivery.</p>
<p>Here's a few useful links relating auditing to DevOps and continuous delivery:</p>
<p><a href="https://www.brightline.com/2012/12/auditing-devops-developers-with-access-to-production/" class="uri">https://www.brightline.com/2012/12/auditing-devops-developers-with-access-to-production/</a></p>
<p><a href="http://itrevolution.com/audit-101-for-devops-resource-guide-for-the-phoenix-project-part-3-correctly-scoping-it-using-gait-and-gait-r/" class="uri">http://itrevolution.com/audit-101-for-devops-resource-guide-for-the-phoenix-project-part-3-correctly-scoping-it-using-gait-and-gait-r/</a></p>
<p><a href="http://www.slideshare.net/SimonStorm1/agile-and-continuous-delivery-for-audits-and-exams-dc-cd-20140528-clean" class="uri">http://www.slideshare.net/SimonStorm1/agile-and-continuous-delivery-for-audits-and-exams-dc-cd-20140528-clean</a></p>
<h3 id="change-management">Change Management</h3>
<p>In order for any organization to have confidence in the quality of the products or services there needs to be assurance that changes are managed and tested during all stages of the development and delivery flow. In some cases external process standards (SOX, FedRAMP, etc.) may dictate how change is actually handled from the identification to delivery. In these cases auditors may struggle to see how tuning processes is decoupled from product/service changes.</p>
<h3 id="separation-of-duties">Separation of Duties</h3>
<p>While it appears inconsistent with DevOps mantras and a flat culture, the notion of Separation of Duties is really just to ensure that accountability is applied to the ‘right’ personnel. Consider the tasks associated with release management, and a processes that requires a human push the final button to do the release. This one final step ensures that developers can’t get software to ‘production’ without some final approval authority. This obviously creates a single point of failure, as well as a potential bottleneck for changes. But, what if you prove the QA processes (tests chains) and can show the manifests for all changes with the artifacts that are moved to production? It is then possible to automate the release step - moving the accountability to where it belongs (developers and operators) while still having the auditability required. And, developers still don’t have access to ‘production’, which is managed by the appropriate personnel.</p>
<h3 id="separation-of-systems">Separation of Systems</h3>
<p>There is no one process for all systems’ requirements. Within an enterprise, there are many different systems in play, and not all treated the same. Financial and personnel management systems don’t require the same intensity or even auditing controls as the corporate wiki. The same can be said for the different pipelines associated with systems under development. Microservices architectures help here, as service functionality can be decoupled and applied to different operational and auditing environments where necessary.</p>
<p>Network infrastructures become the lowest common denominator quickly. Just as systems and processes for the engineering workflows require monitoring and auditing, so does the core infrastructure.</p>
<h3 id="password-and-access-control">Password and Access Control</h3>
<p>While it should go without stating, access control is a critical aspect of any IT operation, including development activities. Access to source code, test cases, and any documentation must be controlled. And just because source code may be open source does not mean that it is without control.</p>
<p>Test servers, network gear and the production pipeline tools all should be monitored from an access perspective. In addition the access control matrix should be reviewed to ensure that only required personnel have access to required systems.</p>
<p>As personnel come and go the access control matrix must be continuously revisited. If possible the database/documents should be monitored, and any change to the them should be reported to lead personnel.</p>
<h3 id="cicd-automation-security">CI/CD Automation Security</h3>
<p>Test environments that are part of the delivery chain must be guarded, as they contain the enterprise’s gold. Much like banks use armored vehicles to transport cash and assets, software artifacts must be ensured the same kinds of security. The systems must be controlled, and any change to them must be tracked and logged appropriately, with notifications to all stakeholders as well.</p>
<p>There are a few tools<a href="#fn29" class="footnoteRef" id="fnref29"><sup>29</sup></a> popping up that can test the security of the build/test environment, and can be an integral part of the CI environment to ensure that nothing has changed there. In addition, typical configuration management tools can be used in coordination with container tools to always use clean machines for building - removing the opportunity for tampering.</p>
<h3 id="process-review">Process Review</h3>
<p>The modern software engineering enterprise, as it is based on constant change, must be equipped with continuous improvement capabilities across the board. But this also requires that the quality assurance must be constantly evolving as well. Process will continue to improve as tooling and practices improve as well. Quality assurance must create a local culture of improvement and level of efforts to help developers and operators achieve business objectives.</p>
<h2 id="logging">Logging</h2>
<p>Automation for auditing is fed by the generation of logs from all systems and transactions against them - for the pipeline. Tools can process the logs, keying on specific events that might trigger and notification/further activity from quality assurance staff. Not only do systems need to generate logs, but transactions between systems need to be monitored as well.</p>
<h2 id="legacy">Legacy</h2>
<p>It is extremely difficult to project when an application should go terminal, get replaced, rewritten, maintained, or just let it run. However, if the development and maintenance teams struggle to implement, or release features and fixes - based on technology in play, it is time to start the decision process. The enterprise will risk market opportunity, and customer satisfaction by holding on to technical debt too long.</p>
<p>Key point: don’t hang on too long<a href="#fn30" class="footnoteRef" id="fnref30"><sup>30</sup></a> - technical debt is a burden on the culture, as well as the bottom line. It is extremely difficult for developers to maintain currency with languages and tools. Having to support older technologies adds to the complexities for developers and operations staff.</p>
<p></p>
<h1 id="technology-services-and-tools">Technology: Services and Tools</h1>
<p>The modern software engineer’s toolbox isn’t that different than what would’ve been found in the past. However, if we head in the direction of the &quot;right tool&quot; for the job, then things get a little crowded. Moreover, the breadth of skills required in a software engineering enterprise create a lot of specialty tools - many of which are brought in and then replaced by the next “new thing”. But, as was discussed above, the need to continuously move forward shouldn’t be eschewed in favor of simpler or more assumed control.</p>
<p>For most enterprises a policy of BYOT (bring your own tools) doesn’t make sense, however it’s not beyond rational for most projects. Developers, like artists, become more efficient with tools they are comfortable with. This is one of the key tenets of PaaS infrastructure services. They can allow developers with a simple way of abstracting common pieces from components that are used to deliver user-facing functionality. For example, it does make sense for an enterprise to down-select a relational database system such as Oracle’s or PostgreSQL, and provide that universally through simple single-purpose database servers in a PaaS delivery environment. Developers of mobile and web-based applications then just need the connection interfaces exposed appropriately for their use - not having to maintain any databases.</p>
<p>This ‘self-service’-focused approach also helps with testing, as it becomes possible for the integration testing to be optimized to a single database server and version - or at least to minimize the variables.</p>
<p>Self-service sandboxes also need to be available for development and testing so developers can evaluate technologies as they become available - or release new versions. By allowing developers and operators the ability to investigate with as little cost or overhead, better awareness and thus decisions about technologies can be made.</p>
<p>The modern software engineering landscape is broad, and deep even with the simplest of projects in tow. It is extremely important that organizations manage services and tools centrally, and enterprise-wide where applicable - yet provide projects with levels of flexibility enabling them to optimize their practices. Deep consideration for both physical and virtual workspaces must be given to ensure engineering teams can communicate and focus as needed</p>
<p></p>
<h2 id="vendor-independence-tool-independence">Vendor Independence, Tool Independence</h2>
<p>We often here about vendor-lock when concerning top-level systems that lock a consumer in, via a proprietary interface or storage pattern. The same concerns for production systems regarding vendor lock, exist also for tools in the engineering chain. Just as system software is evolving at a rapid pace, resolving vulnerabilities and adding new features - tools are constantly adding capabilities, enhancements and fixes too. New tools replace existing tools for reasons. Tool technology can very quickly become technical debt, especially when continuous delivery is the objective. Open source helps this, as it is easier to see where the technology is going. Proprietary tool manufacturers can hook the enterprise, then slack on updating or staying current with expected features.</p>
<p>Currently, the testing arena is advancing rapidly in part due to new workflow automation capabilities being combined with cloud platforms. Tools for mobile development are also rapidly coming to market. This makes it very difficult for the engineering enterprise to down-select and make tools available for engineers to use. And it is even more difficult for practitioners to stay current. Just as systems abstraction is good in architecture and design, so to does it apply to tool integrations.</p>
<p>Many software projects can be built in basic form, from the command line, using basic scripting technology. This is important for a couple of reasons. One, the scripts can be executed by machines (in continuous integration environments for example), and two it allows different tools to be used by developers in the coding and documenting process.</p>
<p>External providers too, such as public cloud providers, have become commonplace in engineering teams. It has become really easy to depend on a provider for various critical pieces of the engineering process. The challenge is to not be locked into their interfaces to the point where a change makes sense - for new features somewhere else, pricing changes, or just because the current provider can't support the team's needs.</p>
<p>It's generally best to stick to standards - where possible, even if just de facto. The chances of getting caught in a place where changing is complex and expensive is reduced.</p>
<p></p>
<h2 id="enterprise-wide">Enterprise-Wide</h2>
<p>Oversight is always hard, especially in large organizations. The complexity of software projects doesn’t make it any easier - there are so many technologies coming and going. It is extremely important for the engineering enterprise to provide a level control to the distribution of projects and teams. Controlled but not totalitarian. Project teams must be empowered to make decisions on technology choices - informed decisions.</p>
<p>There is a solid consensus these days that issue tracking<a href="#fn31" class="footnoteRef" id="fnref31"><sup>31</sup></a>, version control<a href="#fn32" class="footnoteRef" id="fnref32"><sup>32</sup></a> and continuous integration<a href="#fn33" class="footnoteRef" id="fnref33"><sup>33</sup></a> are absolute services, required in the enterprise offering. It is best that these do exist and get maintained at an enterprise level. Maintaining such systems per project adds overhead to teams that should be focused on solving problems. Also, it’s important to provide a universal workflow to teams, so engineers and managers who transition on, or to another team get a familiar experience.</p>
<p>Self-service is a must, at a minimum developers must have an easy path to request capabilities and/or update paths for software.</p>
<p>Operational environments must be well understood top-to-bottom to ensure the developers and security staff generate and work against realistic requirements.</p>
<h3 id="ephemeralization">Ephemeralization</h3>
<p>As the need to do more with less infiltrates every organization so does the need for the infrastructure to enable it. In most cases less comes in the form of abstractions, where work is hidden in layers of simplified interfaces. This risk with over-abstraction is the increase in interface complexity, and inability to accommodate changes up or down in the stack. Adam Wiggins, founder of Heroku puts it in the context of <a href="http://adam.herokuapp.com/past/2011/4/7/ephemeralization/">Machetes, not Swiss-Army knives</a> which accurately describes the needs-to-solutions ratio. Tools must be flexible to support many needs, not singular tools for singular solutions that create typical IT maintenance nightmares.</p>
<h3 id="testing-toolkits-and-infrastructure">Testing Toolkits and Infrastructure</h3>
<p>For the same reasons as having a common issue tracking tool, it is necessary to manage a centralized functional testing environment. Regardless of whether you’re developing a simple web app, mobile interface, or a highly complex desktop application - the benefit of having a common suite are test reuse, and test-writing optimization. From an operations perspective, having a common suite (meaning tools for each target down-selected) test resources can also be reused, or made dynamic, provisioning only when a test operation is required. In some cases, tests may be externalized - running on public (secured) infrastructure when demand requires.</p>
<h3 id="continuous-integration-and-the-pipeline">Continuous Integration and the Pipeline</h3>
<p>Continuous integration services are prime for cloud deployment. In addition, the elasticity provided by IaaS services allows for all build and test targets to be provisioned, configuration and run in parallel, as demand requires.</p>
<h3 id="cloud-public-and-private">Cloud (Public AND Private)</h3>
<p>Cloud ain't cheap not easy, not for production, or for development, or for test, or for maintenance: all have an impact on the bottom line. It isn’t easy to develop for either - at least in order to take advantage of any provider’s features. And then there’s cloud-lock. While Amazon’s API is the de facto standard, and most other providers accommodate it to simplify transition to their services. All of the &quot;cloud&quot; libraries do Amazon API, and a slew of others, so from an application development perspective it IS possible to garner a bit of abstraction. But, to really take advantage of a providers’ features (elasticity, deployment, monitoring, etc.) you’re going to get into the weeds quickly - and thus cloud locked.</p>
<p>That are however enough good reasons for developers having access to a self-service replica of the production environment - to sandbox concepts, new software dependencies and to prove out performance updates. Virtual machines are a step in the right direction and when combined with configuration management techniques can prove really useful - as long as they supported enterprise wide. That said desktop virtualization is only as good as the continuous delivery chain, and the ability for machines and configuration to be distributed.</p>
<p>The discussion of private versus public is well beyond the scope of this paper. However, it should be stated there are pros and cons to both from a development environment perspective. Public cloud services like IaaS and PaaS can be very accommodating to development environments, and proof of concepts - especially where the engineering teams are geographically separated.</p>
<h3 id="distributed-version-control-system">Distributed Version Control System</h3>
<p>This one is almost an absolute. Aside from the reasons associated with intellectual property and access controls having all source code managed centrally allows for discovery, static analysis, and uniform tooling. Most importantly a distributed version control system (DVCS) combined with a social front-end (e.g. Github or Bitbucket) can be used to establish a culture of sharing and collaboration.</p>
<h3 id="artifact-control-general-and-language-specific">Artifact Control (general and language-specific)</h3>
<p>As discussed in the Process chapter, artifact repositories are required to aid in the establishment of solid and secure ingress pipelines. They also allow the enterprise to control components for consumption. For example, if an artifact version is known to have vulnerabilities it can be red-flagged and prevented from being consumed. Central repositories also enable simple notification channels for engineers on the availability of new versions.</p>
<h3 id="monitoring">Monitoring</h3>
<p>Feedback to developers is critical in identifying operational issues, regardless of the architecture. However, the architecture itself must be monitored - across interfaces, to ensure there are no cross-system issues (at any time, or over time - trends).</p>
<p></p>
<h2 id="project-specific">Project-Specific</h2>
<p>Flexibility in enterprise processes and tools is required to ensure that square projects aren’t forced into round holes. However, all projects should conform to the security and artifact delivery requirements expected and specified by customers. In many cases each project will have specific target (where ever the software is supposed to run) requirements. It is essential that the engineering team have access to custom workflows that can instantiate the target-specific environments for development and testing - most likely to include configuration management manifests for the machines in that environment.</p>
<p>Just as important to the local development environment, the physical environment must be optimized for efficiency. The environments should be optimized to allow engineer communications and messaging, as well as allow for quick state reviews without worrying about simple things like conference room availabilities.</p>
<p>As it is possible that the modern software engineering environment is geographically distributed, the projects need to ensure they are supported by enterprise-managed communications tools.</p>
<h3 id="delivery-requirements">Delivery requirements</h3>
<p>It isn't abnormal to expect that each customer have unique requirements for delivery of software artifacts, and in some cases there may be multiple paths for delivery - based on different sets of deliverables. In all cases the workflow should be documented, and optimized for efficiency and repeatability - and automated as much as possible.</p>
<p>The documentation should be used as an entry point for continuous improvement and auditing activities.</p>
<p>As mentioned in the section on Agile it is really important that all processes be reviewed. And so should any automation that is built to support processes. Where possible, automated notifications should be used to inform engineering team and stakeholders of any automation or process changes made.</p>
<h3 id="target-matrix">Target Matrix</h3>
<p>While the paper discusses continuous integration as an enterprise-wide service that should be provided universally to all projects (as a self-service function) there are definitely cases where project-specific requirements dictate unique machine configurations for testing.</p>
<p>For example, a customer may be required to run software on a specific target platform. Let's use Solaris as a special case target. Enterprise engineering support only provides Windows and Linux machines. In cases such as this, where the engineering team might be the Solaris expertise it may be required that a special purpose Solaris environment be created for continuous integration and testing pipeline specifications. In some cases it might just be a special virtual machine that gets created as a build/test agent for the master continuous integration environment.</p>
<p>One of the most important aspects with regards to targets, is the ability to recreate and rerun specific test cases. With this in mind it is an absolute that target machines get managed via some kind of manifest of base machine that can be reproduced (to the test state).</p>
<p>Because scaling to support permutations of configurations is important, as much of the target provisioning and configuration should be automated.</p>
<h3 id="development-environments">Development Environments</h3>
<p>Deploy on day one! While Etsy, a marketplace for craft goods, has made its position in the microservices vs. monolithic discussion for the latter the bigger discussion is on their ability to rapidly on-ramp new developers<a href="#fn34" class="footnoteRef" id="fnref34"><sup>34</sup></a>. To sum it up Etsy uses packaged virtual machines that contain a complete replica of the production environment plus development tools.</p>
<p>By removing the need for a developer to first select, then install and configure a handful of tools, it becomes possible for anyone to quickly get to issues and tasks at hand. But, there is a lot happening, and that has happened, behind the scenes in order for this opportunity to exist.</p>
<p>Creating a virtual machine isn’t all the complicated, but creating one automatically is. Do you create a new machine every time there is a change to the codebase, environment configuration, or operating system and configuration? Well, yes and that’s why it needs to be automated. But, what if a developer already has the virtual machine, do they need to get a new one with every change? No, of course not...but they will need to update their locale codebase and configuration to match any changes downstream (in production). This done by using by source code management (with a version control system) and configuration management (using a tool like Puppet, Chef, Ansible, etc.). Any changes captured anywhere in the system-whole configuration are applied locally to the VM. This isn’t without challenge, because obviously the system configuration IS different locally - network, hostname, etc. so care must be taken in the configuration being applied to note the differences. In the VM and cloud world almost everything boils down to a DNS/naming issue so the attempt here is to simply trick the local environment into thinking it is production.</p>
<p>Although Etsy’s objectives are being met, they get developers committing on the first day (and even deploying to production), most enterprises are less concerned about developer productivity and more concerned about operations and production optimizations (i.e. scaling). The modern software engineering enterprise is concerned about all the above, as well as being able to provide a path for optimizing the security of upstream dependency changes. Also, while a single monolithic VM makes transporting the &quot;environment&quot; from dev to test and to a production environment a bit simpler, it doesn’t necessarily reflect reality. And transportability isn’t the underlying intent at Etsy, it is simply to remove the burden on developers to maintain a working dev environment. One of the upsides and downsides is that developers can end up effecting system pieces unintentionally. In traditional environments these things are usually trapped during ‘integration’ testing. The downside is that the issue may actually be caused by one of the integration components and not the piece under development. As with everything it is best to evaluate the objectives and requirements, identifying a balance between isolating discrete services and applications, and the optimal container and delivery apparatus.</p>
<p>It is worth pointing out here that the differences really are indeed architectural, but in response to modern operating environments. The availability of cloud resources (public and private, and in some cases both) has made it easier to decouple and componentize services and application functionality with scalability - by design. Even if you were running core services (e.g. databases) on bare metal servers, following a multi-tier architecture - it is possible to provide developers with hosted replicas that stay in sync with production data. And the same can be done for middleware (asynchronous messaging, external processors, etc.).</p>
<p>As long as it IS possible for developers to develop, and commit on day one, we’re stepping in the right direction towards the objective delivery targets.</p>
<h3 id="configuration-management">Configuration Management!</h3>
<p>Briefly mentioned above is the engineering and operational concept of Configuration Management, not to be confused with Change Management or the SEI’s definition<a href="#fn35" class="footnoteRef" id="fnref35"><sup>35</sup></a>. The UK Governments' new initiative around enterprise-side coding is spot on, specifically with their tie to &quot;Infrastructure as Code&quot;<a href="#fn36" class="footnoteRef" id="fnref36"><sup>36</sup></a>. Infrastructure as code is a key principal in being able to manage the relationships between dev, test(s) and production environments. Drift<a href="#fn37" class="footnoteRef" id="fnref37"><sup>37</sup></a>, or the potential for discrepancies between the environments, is a manageable thing, simply by having a high-assurance that each configuration shares the same source definitions (at least a major level, with only minor deltas to accommodate for the actual differences in network and naming configuration). When combined with a solid release strategy, configuration management can complete the “bill of materials” as a service or product goes through the dev to production delivery processes.</p>
<p>There is a good selection of tools available for solving configuration management problems. Some are better than others, but the space is evolving rapidly. Configuration management is a delicate space, and support for the system is essential - both from an external (customer service agreement) and internal (IT-supported) perspective. Internally the modern enterprise should evangelize the activity and the tools that it supports to ensure that development and operations teams alike get the message. Configuration management is new enough that it isn’t completely pervasive.</p>
<p>Configuration management, just like software development, is a coding-based skill. The notion of ‘infrastructure as code’ is just that, treat it like code. This means that the infrastructure is versioned. All of the configuration management tools are code-based, some use domain-specific languages (on top of a scripting language) which can require a bit of learning overhead, no different than any technology dependency. Regardless, proper evaluation is required to ensure that both operations and development staff can develop the configuration, maintain it, and utilize it to configure all environments.</p>
<p>One last point on configuration management: it should be used to baseline configurations, allowing for developers and operators to test changes, evaluating the effects in an automatable way. In most cases this is a simple branching pattern, that developers should be familiar with. Because there are skills and experience in play in configuration management that bridge both development and operations it usually gets lumped into the DevOps world. There’s nothing wrong with this, it is indeed one aspect of DevOps - but an enterprise does not need a DevOp position in order to do configuration management.</p>
<h3 id="integrated-development-environments-ides">Integrated Development Environments (IDEs)</h3>
<p>Many developers rely on an environment that provides a consolidated application for their development activities. The Android and iOS SDKs, Java, and C++/.NET on Windows still are dominated by IDEs for development. While there are multiple IDE options for each of these platforms it is in the enterprises best interest to support only one. While smaller organizations may be able to support a BYOT environment, large scale engineering environments would benefit from the continuity and common documentation that supported a single operational base platform. From there developers can use desktop virtualization to support the actual development toolchain.</p>
<h4 id="vms---vagrant">VMs - Vagrant</h4>
<p>There are a few options for desktop virtualization. For the most part, from a machine perspective there's little difference between them. In addition, just working with machine images misses the point to a degree. Because most development environments are evolving and dynamic, it is important to combine the VM tools with a configuration management toolset - and use configuration manifests to keep machines up-to-date.</p>
<p>Here's a few links covering the greatness that is the tool Vagrant, which allows developers to programmatically control there VM configurations:</p>
<p><a href="https://www.andrewmunsell.com/blog/development-environments-with-vagrant-and-puppet" class="uri">https://www.andrewmunsell.com/blog/development-environments-with-vagrant-and-puppet</a></p>
<p><a href="http://markgoodyear.com/2014/03/better-development-environments-with-vagrant/" class="uri">http://markgoodyear.com/2014/03/better-development-environments-with-vagrant/</a></p>
<p><a href="http://blog.kloudless.com/2013/07/01/automating-development-environments-with-vagrant-and-puppet/" class="uri">http://blog.kloudless.com/2013/07/01/automating-development-environments-with-vagrant-and-puppet/</a></p>
<p>Even Microsoft is in:</p>
<p><a href="http://msopentech.com/blog/2014/03/31/building-development-environment-vagrant-puppet/" class="uri">http://msopentech.com/blog/2014/03/31/building-development-environment-vagrant-puppet/</a></p>
<h4 id="containers-docker">Containers (Docker)</h4>
<p>Container technology is evolving rapidly. And, since this is a paper discussing the modern software engineering enterprise it can't be ignored. Docker<a href="#fn38" class="footnoteRef" id="fnref38"><sup>38</sup></a> has turned the technology world, with an opportunity, with no rival. While virtual machine technology and cloud technologies are the main focus of enterprise infrastructure opportunities, Docker brings modern containerization to the masses - in a mainly open source way. And for the most part Docker is playing an industry darling with major players such as Google and RedHat primed with their products using Docker. Simply put, containers simplify the abstraction between a virtual machine and the host operating system, using more of the latter to provide capabilities inside the machine. This shift of responsibility makes containers really lightweight, portable and application focused.</p>
<p>While some organizations are already operationalizing Docker, most are using it as a developer tool, to simply the local environment. This makes sense, and it always creates an on-ramp as containers can be used as a delivery apparatus between dev and various stages of testing. Docker is also logical choice for basic continuous integration activities. While Docker is Linux-only, combining it with a CI server allows for elastic build agents.</p>
<p>Here's a few links to material on Docker as a dev environment:</p>
<p><a href="http://www.drdobbs.com/architecture-and-design/containers-for-development/240168801" class="uri">http://www.drdobbs.com/architecture-and-design/containers-for-development/240168801</a></p>
<p><a href="http://www.slideshare.net/jpetazzo/docker-and-containers-for-development-and-deployment-scale12x" class="uri">http://www.slideshare.net/jpetazzo/docker-and-containers-for-development-and-deployment-scale12x</a></p>
<p><a href="http://www.fig.sh" class="uri">http://www.fig.sh</a></p>
<h3 id="the-physical-environment">The Physical Environment</h3>
<p>Engineers are special, and good ones are exceptional. In order to hire, and keep, good talent it is essential that the physical working space accommodate their needs (and their team's needs). In addition, it’s important to make generic information available to engineer’s passively. Wall mounted monitors are a great tool for this. Keeping engineers in tune with organizational efforts is also a good way to identify opportunities for collaboration. While the intent should not be to gamify developer activities, it is possible to use basic statistics about workflow progress and application performance as a way to incentivize continuous delivery and improvement. Sandbox resources should also be available - tools for personal experiments and a generic playground for inviting creativity that's relative to the products and services being engineered.</p>
<p>There are a few open discussions[^projectspec] about what is actually required for developer effectiveness and efficiency. Multiple monitors connected to a high powered laptop are an absolute must. Screen real estate is as important is the physical desktop. As the modern development environment includes virtual machines the laptop must be powerful enough to run its host OS, plus a couple others (memory and CPU). The ability to be mobile is essential, developers are much more likely to work hard at different times - some of which might be away from the desk.</p>
<h3 id="the-distributed-environment">The Distributed Environment</h3>
<p>There are many reasons for a major up-tick in the number of professionals working remotely. The biggest justification for enterprises is that it is just required to maintain a quality workforce. Many employees are unwilling to relocate, and are looking for opportunities to commute less. And in some cases enterprises just have facilities in many geographically distributed areas - or even globally. From a technical perspective there are very few justifications for having employees in the same physical location. The biggest reason is for operational security, and protection of the codebase. Even with virtual private networking capabilities it is very difficult to ensure the security of both the local engineering environment and enterprise services. In most cases, unless there is critical security, as in National, requirements remote environments are secure enough.</p>
<p>One of the challenges inherent in remote work, is managing inclusiveness. If one member is remote, while the rest of the team shares a physical office space, it can become very difficult for the remote working to stay engaged, especially as ad-hoc activities are easy for those in the office. The team, specifically the leadership must work to engage the remote staff at all times.</p>
<p>Another challenge is when any of the team members are spread across timezones. Scheduling meetings, in particular recurring ones (e.g. daily standups), can become difficult as the most logical time is early or late in the day. The question becomes &quot;whose day?&quot;.</p>
<p>The virtual office isn’t for everyone<a href="#fn39" class="footnoteRef" id="fnref39"><sup>39</sup></a>. There are many challenges on both sides - but most are personal and cultural. There are a handful of tools that can help manage the challenges from both sides - first is a central issue/bug tracking system. Aside from the need to track and collect knowledge on the project itself, the centralized issue tracker serves as a basic time and effort management system. Effort is one of the greatest factors in the teleworking group dynamic.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="http://online.wsj.com/news/articles/SB10001424053111903480904576512250915629460" class="uri">http://online.wsj.com/news/articles/SB10001424053111903480904576512250915629460</a><a href="#fnref1">↩</a></p></li>
<li id="fn2"><p><a href="http://nvd.nist.gov/home.cfm" class="uri">http://nvd.nist.gov/home.cfm</a><a href="#fnref2">↩</a></p></li>
<li id="fn3"><p><a href="http://www.drdobbs.com/software-change-management/184415707" class="uri">http://www.drdobbs.com/software-change-management/184415707</a><a href="#fnref3">↩</a></p></li>
<li id="fn4"><p><a href="http://computingcareers.acm.org/?page_id=12" class="uri">http://computingcareers.acm.org/?page_id=12</a><a href="#fnref4">↩</a></p></li>
<li id="fn5"><p><a href="http://education-portal.com/academy/lesson/matrix-organizational-structure-advantages-disadvantages-examples.html#lesson" class="uri">http://education-portal.com/academy/lesson/matrix-organizational-structure-advantages-disadvantages-examples.html#lesson</a><a href="#fnref5">↩</a></p></li>
<li id="fn6"><p><a href="http://en.wikipedia.org/wiki/The_Mythical_Man-Month#Code_freeze_and_system_versioning" class="uri">http://en.wikipedia.org/wiki/The_Mythical_Man-Month#Code_freeze_and_system_versioning</a><a href="#fnref6">↩</a></p></li>
<li id="fn7"><p><a href="https://www.cs.cmu.edu/~xia/resources/Documents/cscw2012_Github-paper-FinalVersion-1.pdf" class="uri">https://www.cs.cmu.edu/~xia/resources/Documents/cscw2012_Github-paper-FinalVersion-1.pdf</a><a href="#fnref7">↩</a></p></li>
<li id="fn8"><p><a href="http://www.cloudave.com/493/six-factors-in-emergent-innovation/" class="uri">http://www.cloudave.com/493/six-factors-in-emergent-innovation/</a><a href="#fnref8">↩</a></p></li>
<li id="fn9"><p><a href="http://www.jarche.com/2013/03/the-knowledge-sharing-paradox/" class="uri">http://www.jarche.com/2013/03/the-knowledge-sharing-paradox/</a><a href="#fnref9">↩</a></p></li>
<li id="fn10"><p><a href="http://techcrunch.com/2014/07/10/our-polyglot-nightmare/" class="uri">http://techcrunch.com/2014/07/10/our-polyglot-nightmare/</a><a href="#fnref10">↩</a></p></li>
<li id="fn11"><p><a href="http://jondavidjohn.com/where-polyglot-programming-goes-awry/" class="uri">http://jondavidjohn.com/where-polyglot-programming-goes-awry/</a><a href="#fnref11">↩</a></p></li>
<li id="fn12"><p><a href="http://theagileadmin.com/what-is-devops/" class="uri">http://theagileadmin.com/what-is-devops/</a><a href="#fnref12">↩</a></p></li>
<li id="fn13"><p><a href="https://pragprog.com/book/pad/practices-of-an-agile-developer" class="uri">https://pragprog.com/book/pad/practices-of-an-agile-developer</a><a href="#fnref13">↩</a></p></li>
<li id="fn14"><p><a href="http://blog.codinghorror.com/paying-down-your-technical-debt/" class="uri">http://blog.codinghorror.com/paying-down-your-technical-debt/</a><a href="#fnref14">↩</a></p></li>
<li id="fn15"><p><a href="http://alarmingdevelopment.org/?p=893" class="uri">http://alarmingdevelopment.org/?p=893</a><a href="#fnref15">↩</a></p></li>
<li id="fn16"><p><a href="http://codesqueeze.com/the-7-software-ilities-you-need-to-know/" class="uri">http://codesqueeze.com/the-7-software-ilities-you-need-to-know/</a><a href="#fnref16">↩</a></p></li>
<li id="fn17"><p><a href="http://12factor.net" class="uri">http://12factor.net</a><a href="#fnref17">↩</a></p></li>
<li id="fn18"><p><a href="http://continuousdelivery.com/2014/02/visualizations-of-continuous-delivery/" class="uri">http://continuousdelivery.com/2014/02/visualizations-of-continuous-delivery/</a><a href="#fnref18">↩</a></p></li>
<li id="fn19"><p><a href="http://puppetlabs.com/blog/continuous-delivery-vs-continuous-deployment-whats-diff" class="uri">http://puppetlabs.com/blog/continuous-delivery-vs-continuous-deployment-whats-diff</a><a href="#fnref19">↩</a></p></li>
<li id="fn20"><p><a href="http://softwaretestingfundamentals.com/verification-vs-validation/" class="uri">http://softwaretestingfundamentals.com/verification-vs-validation/</a><a href="#fnref20">↩</a></p></li>
<li id="fn21"><p><a href="https://www.scrum.org/Resources/Scrum-Glossary/Definition-of-Done" class="uri">https://www.scrum.org/Resources/Scrum-Glossary/Definition-of-Done</a><a href="#fnref21">↩</a></p></li>
<li id="fn22"><p><a href="http://msdn.microsoft.com/en-us/library/aa730844(v=vs.80).aspx" class="uri">http://msdn.microsoft.com/en-us/library/aa730844(v=vs.80).aspx</a><a href="#fnref22">↩</a></p></li>
<li id="fn23"><p><a href="http://dannorth.net/introducing-bdd/" class="uri">http://dannorth.net/introducing-bdd/</a><a href="#fnref23">↩</a></p></li>
<li id="fn24"><p><a href="http://www.itil-officialsite.com/aboutitil/whatisitil.aspx" class="uri">http://www.itil-officialsite.com/aboutitil/whatisitil.aspx</a><a href="#fnref24">↩</a></p></li>
<li id="fn25"><p><a href="http://itilcollege.com/DefinitiveSoftwareLibraryITIL.html" class="uri">http://itilcollege.com/DefinitiveSoftwareLibraryITIL.html</a><a href="#fnref25">↩</a></p></li>
<li id="fn26"><p><a href="http://www.cio.com/article/2431556/developer/5-reasons-for-software-developers-to-do-code-reviews--even-if-you-think-they-re-a-waste-of.html" class="uri">http://www.cio.com/article/2431556/developer/5-reasons-for-software-developers-to-do-code-reviews--even-if-you-think-they-re-a-waste-of.html</a><a href="#fnref26">↩</a></p></li>
<li id="fn27"><p><a href="http://www.sei.cmu.edu/cmmi/" class="uri">http://www.sei.cmu.edu/cmmi/</a><a href="#fnref27">↩</a></p></li>
<li id="fn28"><p><a href="https://www.brightline.com/2012/12/auditing-devops-developers-with-access-to-production/" class="uri">https://www.brightline.com/2012/12/auditing-devops-developers-with-access-to-production/</a><a href="#fnref28">↩</a></p></li>
<li id="fn29"><p><a href="https://github.com/claudijd/rotten_apple" class="uri">https://github.com/claudijd/rotten_apple</a><a href="#fnref29">↩</a></p></li>
<li id="fn30"><p><a href="http://www.sei.cmu.edu/community/td2013/program/upload/technicaldebt-icse.pdf" class="uri">http://www.sei.cmu.edu/community/td2013/program/upload/technicaldebt-icse.pdf</a><a href="#fnref30">↩</a></p></li>
<li id="fn31"><p><a href="http://mashable.com/2014/02/16/bug-tracking-apps/" class="uri">http://mashable.com/2014/02/16/bug-tracking-apps/</a><a href="#fnref31">↩</a></p></li>
<li id="fn32"><p><a href="http://en.wikipedia.org/wiki/Revision_control" class="uri">http://en.wikipedia.org/wiki/Revision_control</a><a href="#fnref32">↩</a></p></li>
<li id="fn33"><p><a href="http://www.extremeprogramming.org/rules/integrateoften.html" class="uri">http://www.extremeprogramming.org/rules/integrateoften.html</a><a href="#fnref33">↩</a></p></li>
<li id="fn34"><p><a href="http://codeascraft.com/2012/03/13/making-it-virtually-easy-to-deploy-on-day-one/" class="uri">http://codeascraft.com/2012/03/13/making-it-virtually-easy-to-deploy-on-day-one/</a><a href="#fnref34">↩</a></p></li>
<li id="fn35"><p><a href="https://www.sei.cmu.edu/productlines/frame_report/config.man.htm" class="uri">https://www.sei.cmu.edu/productlines/frame_report/config.man.htm</a><a href="#fnref35">↩</a></p></li>
<li id="fn36"><p><a href="https://www.gov.uk/service-manual/making-software/configuration-management.html#infrastructure-as-code" class="uri">https://www.gov.uk/service-manual/making-software/configuration-management.html#infrastructure-as-code</a><a href="#fnref36">↩</a></p></li>
<li id="fn37"><p><a href="http://java.dzone.com/articles/infrastructure-code-when" class="uri">http://java.dzone.com/articles/infrastructure-code-when</a><a href="#fnref37">↩</a></p></li>
<li id="fn38"><p><a href="http://www.techrepublic.com/article/why-docker-and-why-now/" class="uri">http://www.techrepublic.com/article/why-docker-and-why-now/</a><a href="#fnref38">↩</a></p></li>
<li id="fn39"><p><a href="http://www.techrepublic.com/blog/10-things/10-signs-that-you-arent-cut-out-to-be-a-telecommuter/" class="uri">http://www.techrepublic.com/blog/10-things/10-signs-that-you-arent-cut-out-to-be-a-telecommuter/</a><a href="#fnref39">↩</a></p></li>
</ol>
</div>
</body>
</html>
